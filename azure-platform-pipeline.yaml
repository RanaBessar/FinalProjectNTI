# ============================================================================
# Azure DevOps Platform Pipeline - DevOps Tools Deployment
# ============================================================================
# This pipeline deploys platform tools: Nginx Ingress, ArgoCD, Vault, SonarQube, Nexus
# Author: DevOps Team
# Version: 2.0.0
# ============================================================================

trigger:
  branches:
    include:
      - main
  paths:
    include:
      - helm/vault/**
      - helm/sonarqube/**
      - helm/nginx-ingress-controller/**
      - helm/*-ingress.yaml
      - gp3-sc.yaml
      - azure-platform-pipeline.yaml

pr:
  branches:
    include:
      - main
  paths:
    include:
      - helm/**

pool:
  name: SelfHostedPool
  demands:
    - agent.os -equals Linux

# ================================
# Pipeline Variables
# ================================
variables:
  # Infrastructure
  AWS_DEFAULT_REGION: "us-east-1"
  CLUSTER_NAME: "nti-final-nonprod-eks"
  
  # Helm
  HELM_WORKING_DIR: "helm"
  HELM_TIMEOUT: "15m"
  
  # Feature Flags
  DEPLOY_INGRESS: "true"
  DEPLOY_ARGOCD: "true"
  DEPLOY_VAULT: "true"
  DEPLOY_SONARQUBE: "true"
  DEPLOY_NEXUS: "true"
  RUN_CLEANUP: "false"
  
  # Retry Configuration
  MAX_RETRIES: 3
  RETRY_DELAY: 30

# ================================
# Stages
# ================================
stages:

# ================================
# Prerequisites Stage
# ================================
- stage: Prerequisites
  displayName: "ðŸ”§ Prerequisites & Validation"
  jobs:
    - job: ValidateEnvironment
      displayName: "Validate Environment"
      timeoutInMinutes: 15
      
      steps:
        - checkout: self
          clean: true
          fetchDepth: 1

        - script: |
            echo "====================================="
            echo "  Environment Validation"
            echo "====================================="
            echo "Pipeline Run ID: $(Build.BuildId)"
            echo "Branch: $(Build.SourceBranchName)"
            echo "Agent: $(Agent.Name)"
            echo "====================================="
          displayName: "Pipeline Information"

        - script: |
            set -e
            echo "Validating AWS credentials..."
            aws --version
            aws sts get-caller-identity
            echo "âœ… AWS credentials validated successfully"
          displayName: "Validate AWS Credentials"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

        - script: |
            set -e
            echo "Validating cluster connectivity..."
            
            mkdir -p $HOME/.kube
            aws eks update-kubeconfig \
              --region "$(AWS_DEFAULT_REGION)" \
              --name "$(CLUSTER_NAME)" \
              --kubeconfig "$HOME/.kube/config"
            
            export KUBECONFIG="$HOME/.kube/config"
            
            kubectl cluster-info
            kubectl get nodes -o wide
            
            NODE_COUNT=$(kubectl get nodes --no-headers | wc -l)
            if [ "$NODE_COUNT" -lt 1 ]; then
              echo "âŒ No nodes available in the cluster!"
              exit 1
            fi
            
            echo "âœ… Cluster validation successful - $NODE_COUNT nodes ready"
          displayName: "Validate Cluster Connectivity"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

        - script: |
            set -e
            export KUBECONFIG="$HOME/.kube/config"
            
            echo "====================================="
            echo "  Verify EBS CSI Driver"
            echo "====================================="
            
            # Check if EBS CSI driver is installed
            if kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-ebs-csi-driver --no-headers 2>/dev/null | grep -q "Running"; then
              echo "âœ… EBS CSI Driver is running"
            else
              echo "âš ï¸ EBS CSI Driver not found or not running"
              echo "Checking EKS addon status..."
              aws eks describe-addon --cluster-name $(CLUSTER_NAME) --addon-name aws-ebs-csi-driver --region $(AWS_DEFAULT_REGION) || true
              
              echo ""
              echo "Installing EBS CSI Driver addon..."
              
              # Get the OIDC provider ID
              OIDC_ID=$(aws eks describe-cluster --name $(CLUSTER_NAME) --region $(AWS_DEFAULT_REGION) --query "cluster.identity.oidc.issuer" --output text | cut -d '/' -f 5)
              
              # Install the addon
              aws eks create-addon \
                --cluster-name $(CLUSTER_NAME) \
                --addon-name aws-ebs-csi-driver \
                --region $(AWS_DEFAULT_REGION) || aws eks update-addon \
                --cluster-name $(CLUSTER_NAME) \
                --addon-name aws-ebs-csi-driver \
                --region $(AWS_DEFAULT_REGION) || true
              
              echo "Waiting for EBS CSI Driver to be ready..."
              sleep 60
              
              kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-ebs-csi-driver
            fi
          displayName: "Verify/Install EBS CSI Driver"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

        - script: |
            set -e
            export KUBECONFIG="$HOME/.kube/config"
            
            echo "====================================="
            echo "  Configure Storage Classes"
            echo "====================================="
            
            # Apply GP3 storage class
            kubectl apply -f gp3-sc.yaml
            
            # Verify storage classes
            echo ""
            echo "Available Storage Classes:"
            kubectl get sc
            
            # Check default storage class
            DEFAULT_SC=$(kubectl get sc -o jsonpath='{.items[?(@.metadata.annotations.storageclass\.kubernetes\.io/is-default-class=="true")].metadata.name}')
            echo ""
            echo "Default Storage Class: $DEFAULT_SC"
            
            if [ "$DEFAULT_SC" != "gp3" ]; then
              echo "âš ï¸ Setting gp3 as default storage class..."
              
              # Remove default annotation from other storage classes
              for sc in $(kubectl get sc -o jsonpath='{.items[*].metadata.name}'); do
                if [ "$sc" != "gp3" ]; then
                  kubectl annotate sc $sc storageclass.kubernetes.io/is-default-class- --overwrite 2>/dev/null || true
                fi
              done
              
              # Set gp3 as default
              kubectl annotate sc gp3 storageclass.kubernetes.io/is-default-class=true --overwrite
            fi
            
            echo "âœ… Storage classes configured successfully"
          displayName: "Configure Storage Classes"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

# ================================
# Platform Deploy Stage
# ================================
- stage: Platform_Deploy
  displayName: "ðŸš€ Deploy Platform Tools"
  dependsOn: Prerequisites
  condition: succeeded()
  
  jobs:
    - job: DeployPlatformTools
      displayName: "Deploy Platform Components"
      timeoutInMinutes: 90
      
      steps:
        - checkout: self
          clean: true
          fetchDepth: 1

        - task: HelmInstaller@1
          displayName: "Install Helm"
          inputs:
            helmVersionToInstall: "latest"

        - task: KubectlInstaller@0
          displayName: "Install kubectl"
          inputs:
            kubectlVersion: "latest"

        - script: |
            set -e
            mkdir -p $HOME/.kube
            aws eks update-kubeconfig \
              --region "$(AWS_DEFAULT_REGION)" \
              --name "$(CLUSTER_NAME)" \
              --kubeconfig "$HOME/.kube/config"
            export KUBECONFIG="$HOME/.kube/config"
            kubectl get nodes
          displayName: "Configure kubeconfig"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

        - script: |
            set -e
            echo "====================================="
            echo "  Add Helm Repositories"
            echo "====================================="
            
            declare -A REPOS=(
              ["hashicorp"]="https://helm.releases.hashicorp.com"
              ["sonarqube"]="https://SonarSource.github.io/helm-chart-sonarqube"
              ["ingress-nginx"]="https://kubernetes.github.io/ingress-nginx"
              ["sonatype"]="https://sonatype.github.io/helm3-charts"
              ["argo"]="https://argoproj.github.io/argo-helm"
            )
            
            # Remove deprecated stable repo if exists (causes timeouts)
            helm repo remove stable 2>/dev/null || true
            
            for repo in "${!REPOS[@]}"; do
              if helm repo list | grep -q "^$repo"; then
                echo "Updating $repo..."
                helm repo update $repo
              else
                echo "Adding $repo..."
                helm repo add $repo ${REPOS[$repo]}
              fi
            done
            
            echo "âœ… Helm repositories configured"
          displayName: "Add Helm Repositories"

        # ================================
        # Deploy Nginx Ingress Controller
        # ================================
        - script: |
            if [ "$(DEPLOY_INGRESS)" != "true" ]; then
              echo "Skipping Nginx Ingress deployment (DEPLOY_INGRESS=false)"
              exit 0
            fi
            
            set -e
            export KUBECONFIG="$HOME/.kube/config"
            
            echo "====================================="
            echo "  Deploy Nginx Ingress Controller"
            echo "====================================="
            
            helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
              --namespace ingress-nginx \
              --create-namespace \
              --values ./$(HELM_WORKING_DIR)/nginx-ingress-controller/values.yaml \
              --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-type"=nlb \
              --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-scheme"=internet-facing \
              --wait \
              --timeout $(HELM_TIMEOUT) \
              --atomic
            
            echo "Waiting for LoadBalancer..."
            kubectl wait --namespace ingress-nginx \
              --for=condition=available deployment/ingress-nginx-controller \
              --timeout=300s
            
            echo ""
            echo "Ingress Controller Services:"
            kubectl get svc -n ingress-nginx
            
            echo "âœ… Nginx Ingress Controller deployed successfully"
          displayName: "Deploy Nginx Ingress"

        # ================================
        # Deploy ArgoCD
        # ================================
        - script: |
            if [ "$(DEPLOY_ARGOCD)" != "true" ]; then
              echo "Skipping ArgoCD deployment (DEPLOY_ARGOCD=false)"
              exit 0
            fi
            
            set -e
            export KUBECONFIG="$HOME/.kube/config"
            
            echo "====================================="
            echo "  Deploy ArgoCD"
            echo "====================================="
            
            helm upgrade --install argocd argo/argo-cd \
              --namespace argo \
              --create-namespace \
              --set server.service.type=ClusterIP \
              --set server.ingress.enabled=false \
              --set configs.params."server\.insecure"=true \
              --wait \
              --timeout $(HELM_TIMEOUT) \
              --atomic
            
            # Apply ingress
            kubectl apply -f ./$(HELM_WORKING_DIR)/argocd-ingress.yaml
            
            # Wait for ArgoCD to be ready
            kubectl wait --namespace argo \
              --for=condition=available deployment/argocd-server \
              --timeout=300s
            
            echo ""
            echo "ArgoCD Pods:"
            kubectl get pods -n argo
            
            # Get initial admin password
            echo ""
            echo "ArgoCD Admin Password (initial):"
            kubectl -n argo get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
            echo ""
            
            echo "âœ… ArgoCD deployed successfully"
          displayName: "Deploy ArgoCD"

        # ================================
        # Deploy Vault
        # ================================
        - script: |
            if [ "$(DEPLOY_VAULT)" != "true" ]; then
              echo "Skipping Vault deployment (DEPLOY_VAULT=false)"
              exit 0
            fi
            
            set -e
            export KUBECONFIG="$HOME/.kube/config"
            
            echo "====================================="
            echo "  Deploy Vault"
            echo "====================================="
            
            # Clean up any existing webhook configurations that might conflict
            kubectl delete mutatingwebhookconfiguration vault-agent-injector-cfg --ignore-not-found=true
            
            helm upgrade --install vault hashicorp/vault \
              --namespace vault \
              --create-namespace \
              --values ./$(HELM_WORKING_DIR)/vault/values.yaml \
              --set injector.enabled=false \
              --set server.dataStorage.storageClass=gp3 \
              --wait \
              --timeout $(HELM_TIMEOUT)
            
            # Apply ingress
            kubectl apply -f ./$(HELM_WORKING_DIR)/vault-ingress.yaml
            
            echo ""
            echo "Vault Pods:"
            kubectl get pods -n vault
            
            echo "âœ… Vault deployed successfully"
          displayName: "Deploy Vault"

        # ================================
        # Deploy SonarQube
        # ================================
        - script: |
            if [ "$(DEPLOY_SONARQUBE)" != "true" ]; then
              echo "Skipping SonarQube deployment (DEPLOY_SONARQUBE=false)"
              exit 0
            fi
            
            set -e
            export KUBECONFIG="$HOME/.kube/config"
            
            echo "====================================="
            echo "  Deploy SonarQube"
            echo "====================================="
            
            # Delete existing PVC if stuck in Pending state
            PVC_STATUS=$(kubectl get pvc -n sonarqube -o jsonpath='{.items[0].status.phase}' 2>/dev/null || echo "NotFound")
            if [ "$PVC_STATUS" = "Pending" ]; then
              echo "Found pending PVC, cleaning up..."
              helm uninstall sonarqube -n sonarqube --ignore-not-found || true
              kubectl delete pvc --all -n sonarqube --ignore-not-found || true
              sleep 10
            fi
            
            helm upgrade --install sonarqube sonarqube/sonarqube \
              --namespace sonarqube \
              --create-namespace \
              --values ./$(HELM_WORKING_DIR)/sonarqube/values.yaml \
              --set persistence.storageClass=gp3 \
              --set postgresql.persistence.storageClass=gp3 \
              --wait \
              --timeout 20m
            
            # Apply ingress
            kubectl apply -f ./$(HELM_WORKING_DIR)/sonarqube-ingress.yaml
            
            echo ""
            echo "SonarQube Pods:"
            kubectl get pods -n sonarqube
            echo ""
            echo "SonarQube PVCs:"
            kubectl get pvc -n sonarqube
            
            echo "âœ… SonarQube deployed successfully"
          displayName: "Deploy SonarQube"

        # ================================
        # Deploy Nexus
        # ================================
        - script: |
            if [ "$(DEPLOY_NEXUS)" != "true" ]; then
              echo "Skipping Nexus deployment (DEPLOY_NEXUS=false)"
              exit 0
            fi
            
            set -e
            export KUBECONFIG="$HOME/.kube/config"
            
            echo "====================================="
            echo "  Deploy Nexus Repository Manager"
            echo "====================================="
            
            # Check if nexus values file exists
            if [ -f "./$(HELM_WORKING_DIR)/nexus/values.yaml" ]; then
              NEXUS_VALUES="--values ./$(HELM_WORKING_DIR)/nexus/values.yaml"
            else
              NEXUS_VALUES=""
              echo "âš ï¸ No custom values file found, using defaults"
            fi
            
            helm upgrade --install nexus sonatype/nexus-repository-manager \
              --namespace nexus \
              --create-namespace \
              $NEXUS_VALUES \
              --set persistence.storageClass=gp3 \
              --wait \
              --timeout 20m
            
            # Apply ingress if exists
            if [ -f "./nexus-ingress.yaml" ]; then
              kubectl apply -f ./nexus-ingress.yaml
            fi
            
            echo ""
            echo "Nexus Pods:"
            kubectl get pods -n nexus
            echo ""
            echo "Nexus PVCs:"
            kubectl get pvc -n nexus
            
            echo "âœ… Nexus deployed successfully"
          displayName: "Deploy Nexus"

# ================================
# Verification Stage
# ================================
- stage: Verification
  displayName: "âœ… Verify Deployments"
  dependsOn: Platform_Deploy
  condition: succeeded()
  
  jobs:
    - job: VerifyDeployments
      displayName: "Verify All Deployments"
      timeoutInMinutes: 15
      
      steps:
        - checkout: self
          clean: true
          fetchDepth: 1

        - script: |
            set -e
            mkdir -p $HOME/.kube
            aws eks update-kubeconfig \
              --region "$(AWS_DEFAULT_REGION)" \
              --name "$(CLUSTER_NAME)" \
              --kubeconfig "$HOME/.kube/config"
            export KUBECONFIG="$HOME/.kube/config"
          displayName: "Configure kubeconfig"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

        - script: |
            set -e
            export KUBECONFIG="$HOME/.kube/config"
            
            echo "====================================="
            echo "  Platform Deployment Summary"
            echo "====================================="
            
            echo ""
            echo "ðŸ“¦ Namespaces:"
            kubectl get ns | grep -E 'ingress-nginx|argo|vault|sonarqube|nexus' || true
            
            echo ""
            echo "ðŸ”„ All Pods Status:"
            for ns in ingress-nginx argo vault sonarqube nexus; do
              echo ""
              echo "--- $ns ---"
              kubectl get pods -n $ns 2>/dev/null || echo "Namespace not found"
            done
            
            echo ""
            echo "ðŸ’¾ Persistent Volume Claims:"
            kubectl get pvc -A | grep -E 'vault|sonarqube|nexus' || true
            
            echo ""
            echo "ðŸŒ Services:"
            kubectl get svc -A | grep -E 'LoadBalancer|NodePort' || true
            
            echo ""
            echo "ðŸ”— Ingresses:"
            kubectl get ingress -A || true
            
            echo ""
            echo "====================================="
            echo "  Health Check Summary"
            echo "====================================="
            
            FAILED=0
            
            # Check Ingress
            if kubectl get pods -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx --no-headers 2>/dev/null | grep -q "Running"; then
              echo "âœ… Nginx Ingress Controller: HEALTHY"
            else
              echo "âŒ Nginx Ingress Controller: UNHEALTHY"
              FAILED=$((FAILED+1))
            fi
            
            # Check ArgoCD
            if kubectl get pods -n argo -l app.kubernetes.io/name=argocd-server --no-headers 2>/dev/null | grep -q "Running"; then
              echo "âœ… ArgoCD: HEALTHY"
            else
              echo "âŒ ArgoCD: UNHEALTHY"
              FAILED=$((FAILED+1))
            fi
            
            # Check Vault
            if kubectl get pods -n vault --no-headers 2>/dev/null | grep -q "Running"; then
              echo "âœ… Vault: HEALTHY"
            else
              echo "âš ï¸ Vault: Pods not in Running state (may need initialization)"
            fi
            
            # Check SonarQube
            if kubectl get pods -n sonarqube --no-headers 2>/dev/null | grep -q "Running"; then
              echo "âœ… SonarQube: HEALTHY"
            else
              echo "âš ï¸ SonarQube: Starting or unhealthy"
            fi
            
            # Check Nexus
            if kubectl get pods -n nexus --no-headers 2>/dev/null | grep -q "Running"; then
              echo "âœ… Nexus: HEALTHY"
            else
              echo "âš ï¸ Nexus: Starting or unhealthy"
            fi
            
            echo ""
            echo "====================================="
            if [ $FAILED -gt 0 ]; then
              echo "âš ï¸ $FAILED critical components are unhealthy"
              exit 1
            else
              echo "âœ… Platform deployment completed successfully!"
            fi
          displayName: "Verify All Deployments"

# ================================
# Cleanup Stage (Optional)
# ================================
- stage: Platform_Cleanup
  displayName: "ðŸ§¹ Cleanup Platform"
  dependsOn: Verification
  condition: and(succeeded(), eq(variables['RUN_CLEANUP'], 'true'))
  
  jobs:
    - job: CleanupPlatform
      displayName: "Cleanup Platform Components"
      timeoutInMinutes: 30
      
      steps:
        - checkout: self
          clean: true
          fetchDepth: 1

        - task: HelmInstaller@1
          displayName: "Install Helm"
          inputs:
            helmVersionToInstall: "latest"

        - script: |
            set -e
            mkdir -p $HOME/.kube
            aws eks update-kubeconfig \
              --region "$(AWS_DEFAULT_REGION)" \
              --name "$(CLUSTER_NAME)" \
              --kubeconfig "$HOME/.kube/config"
            export KUBECONFIG="$HOME/.kube/config"
          displayName: "Configure kubeconfig"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

        - script: |
            set +e
            export KUBECONFIG="$HOME/.kube/config"
            
            echo "====================================="
            echo "  Cleanup Helm Releases"
            echo "====================================="
            
            for release in nexus sonarqube vault argocd ingress-nginx; do
              NS=$release
              [ "$release" = "argocd" ] && NS="argo"
              [ "$release" = "ingress-nginx" ] && NS="ingress-nginx"
              
              echo "Uninstalling $release..."
              helm uninstall $release -n $NS 2>/dev/null || true
            done
            
            echo ""
            echo "====================================="
            echo "  Cleanup Namespaces"
            echo "====================================="
            
            for ns in nexus sonarqube vault argo ingress-nginx; do
              echo "Deleting namespace $ns..."
              kubectl delete namespace $ns --ignore-not-found=true --timeout=60s || true
            done
            
            echo "âœ… Cleanup completed"
          displayName: "Cleanup Helm + Namespaces"
