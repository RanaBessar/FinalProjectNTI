trigger:
  branches:
    include:
      - main

pool:
  name: self-hosted-pool
  demands:
     - Agent.OS -equals Linux

variables:
  TF_WORKING_DIR: "terraform"
  HELM_WORKING_DIR: "helm"

  AWS_DEFAULT_REGION: "us-east-1"

  TF_STATE_BUCKET: "nti-final-tfstate-842303506852-us-east-1-2026"
  TF_LOCK_TABLE: "nti-finalproject-terraform-lock"

  DEPLOY_HELM: "true"
  RUN_DESTROY: "false"

stages:

# ================================
# Terraform Apply Stage
# ================================
- stage: Terraform_Apply
  displayName: "Terraform Apply Stage"
  jobs:
    - job: TerraformApply
      displayName: "Terraform Apply"
      timeoutInMinutes: 60
      cancelTimeoutInMinutes: 5

      workspace:
        clean: all

      steps:
        - checkout: self

        - script: |
            echo "====================================="
            echo " Agent Health Check"
            echo "====================================="
            echo "Agent Name: $(Agent.Name)"
            echo "Agent OS: $(Agent.OS)"
            echo "Build ID: $(Build.BuildId)"
            df -h
            free -h || true
          displayName: "Agent Health Check"

        - script: |
            echo "====================================="
            echo " Verify Tools"
            echo "====================================="
            aws --version
            terraform --version
            git --version
          displayName: "Verify Tools"

        - script: |
            echo "====================================="
            echo " Validate AWS Credentials"
            echo "====================================="
            aws sts get-caller-identity
          displayName: "Validate AWS Credentials"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

        - script: |
            set -e
            echo "====================================="
            echo " Setup Backend Resources"
            echo "====================================="

            echo "Checking S3 bucket: $(TF_STATE_BUCKET)"
            if aws s3api head-bucket --bucket $(TF_STATE_BUCKET) 2>/dev/null; then
              echo "S3 bucket exists"
            else
              echo "Creating S3 bucket..."
              aws s3api create-bucket --bucket $(TF_STATE_BUCKET) --region $(AWS_DEFAULT_REGION)
              aws s3api put-bucket-versioning --bucket $(TF_STATE_BUCKET) --versioning-configuration Status=Enabled
            fi

            echo "Checking DynamoDB table: $(TF_LOCK_TABLE)"
            if aws dynamodb describe-table --table-name $(TF_LOCK_TABLE) --region $(AWS_DEFAULT_REGION) >/dev/null 2>&1; then
              echo "DynamoDB table exists"
            else
              echo "Creating DynamoDB table..."
              aws dynamodb create-table \
                --table-name $(TF_LOCK_TABLE) \
                --attribute-definitions AttributeName=LockID,AttributeType=S \
                --key-schema AttributeName=LockID,KeyType=HASH \
                --billing-mode PAY_PER_REQUEST \
                --region $(AWS_DEFAULT_REGION)

              echo "Waiting for DynamoDB table..."
              aws dynamodb wait table-exists --table-name $(TF_LOCK_TABLE) --region $(AWS_DEFAULT_REGION)
            fi
          displayName: "Setup Backend Resources"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

        - script: |
            set -e
            cd $(TF_WORKING_DIR)

            echo "====================================="
            echo " Terraform Init"
            echo "====================================="
            terraform init -reconfigure
          displayName: "Terraform Init"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

        - script: |
            set -e
            cd $(TF_WORKING_DIR)

            echo "====================================="
            echo " Terraform Validate"
            echo "====================================="
            terraform validate
          displayName: "Terraform Validate"

        - script: |
            set -e
            cd $(TF_WORKING_DIR)

            echo "====================================="
            echo " Terraform Plan"
            echo "====================================="
            terraform plan -var-file=nonprod.tfvars -out=tfplan
          displayName: "Terraform Plan"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

        - script: |
            set -e
            cd $(TF_WORKING_DIR)

            echo "====================================="
            echo " Terraform Apply"
            echo "====================================="
            terraform apply -auto-approve tfplan
          displayName: "Terraform Apply"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

        - script: |
            cd $(TF_WORKING_DIR)
            echo "====================================="
            echo " Terraform Outputs"
            echo "====================================="
            terraform output
          displayName: "Terraform Outputs"


# ================================
# Helm Deploy Stage
# ================================
- stage: Helm_Deploy
  displayName: "Helm Deployment Stage"
  dependsOn: Terraform_Apply
  condition: and(succeeded(), eq(variables['DEPLOY_HELM'], 'true'))

  jobs:
    - job: HelmDeploy
      displayName: "Helm Deploy"
      timeoutInMinutes: 40
      cancelTimeoutInMinutes: 5

      workspace:
        clean: all

      steps:
        - checkout: self

        - task: HelmInstaller@1
          displayName: "Install Helm"
          inputs:
            helmVersionToInstall: "latest"

        - task: KubectlInstaller@0
          displayName: "Install kubectl"
          inputs:
            kubectlVersion: "latest"

        - script: |
            set -e
            echo "====================================="
            echo " Configure kubeconfig for EKS"
            echo "====================================="

            cd $(TF_WORKING_DIR)
            terraform init -reconfigure

            CLUSTER_NAME=$(terraform output -raw eks_cluster_name)
            echo "Cluster Name: $CLUSTER_NAME"

            echo "Creating kubeconfig directory..."
            mkdir -p $HOME/.kube

            echo "Updating kubeconfig..."
            aws eks update-kubeconfig \
              --region "$(AWS_DEFAULT_REGION)" \
              --name "$CLUSTER_NAME" \
              --kubeconfig "$HOME/.kube/config"

            echo "Testing kubectl..."
            export KUBECONFIG="$HOME/.kube/config"
            kubectl cluster-info
            kubectl get nodes
          displayName: "Configure kubectl for EKS"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

        - script: |
            echo "====================================="
            echo " Check Helm Chart Folder"
            echo "====================================="
            ls -la
            ls -la $(HELM_WORKING_DIR)
            ls -la $(HELM_WORKING_DIR)/nti-app
          displayName: "Check Helm Chart Files"

        - script: |
            set +e
            echo "====================================="
            echo " Add Helm Repositories"
            echo "====================================="

            helm repo add stable https://charts.helm.sh/stable
            helm repo add hashicorp https://helm.releases.hashicorp.com
            helm repo add sonarqube https://SonarSource.github.io/helm-chart-sonarqube
            helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
            helm repo add sonatype https://sonatype.github.io/helm3-charts
            helm repo add argo https://argoproj.github.io/argo-helm
            
            echo "Updating Helm Repositories..."
            helm repo update
            
            if [ $? -eq 0 ]; then
              echo "Helm repositories updated successfully"
            else
              echo "Warning: Some repositories may have failed to update"
            fi
          displayName: "Add Helm Repositories"

        - script: |
            set -e
            export KUBECONFIG="$HOME/.kube/config"
            echo "KUBECONFIG=$KUBECONFIG"

            kubectl get nodes

            echo "====================================="
            echo " Install Nginx Ingress Controller"
            echo "====================================="
            helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
              --namespace ingress-nginx \
              --create-namespace \
              --values ./$(HELM_WORKING_DIR)/nginx-ingress-controller/values.yaml \
              --force \
              --wait \
              --timeout 10m
          displayName: "Deploy Nginx Ingress Controller"

        - script: |
            set -e
            export KUBECONFIG="$HOME/.kube/config"

            echo "====================================="
            echo " Install ArgoCD"
            echo "====================================="
            helm upgrade --install argocd argo/argo-cd \
              --namespace argo \
              --create-namespace \
              --set server.service.type=ClusterIP \
              --set server.ingress.enabled=true \
              --set server.ingress.ingressClassName=nginx \
              --set server.ingress.hosts[0]=argocd.local \
              --set server.ingress.paths[0].path=/ \
              --set server.ingress.paths[0].pathType=Prefix \
              --force \
              --wait \
              --timeout 10m

            kubectl apply -f ./$(HELM_WORKING_DIR)/argocd-ingress.yaml
          displayName: "Deploy ArgoCD"

        - script: |
            set -e
            export KUBECONFIG="$HOME/.kube/config"

            echo "====================================="
            echo " Install Vault"
            echo "====================================="
            
            # Clean up conflicting webhook if it exists
            kubectl delete mutatingwebhookconfigurations vault-agent-injector-cfg --ignore-not-found=true || true
            sleep 5
            
            helm upgrade --install vault hashicorp/vault \
              --namespace vault \
              --create-namespace \
              --values ./$(HELM_WORKING_DIR)/vault/values.yaml \
              --force \
              --wait \
              --timeout 10m

            kubectl apply -f ./$(HELM_WORKING_DIR)/vault-ingress.yaml
          displayName: "Deploy Vault"

        - script: |
            set -e
            export KUBECONFIG="$HOME/.kube/config"

            echo "====================================="
            echo " Install SonarQube"
            echo "====================================="
            helm upgrade --install sonarqube sonarqube/sonarqube \
              --namespace sonarqube \
              --create-namespace \
              --values ./$(HELM_WORKING_DIR)/sonarqube/values.yaml \
              --force \
              --wait \
              --timeout 15m

            kubectl apply -f ./$(HELM_WORKING_DIR)/sonarqube-ingress.yaml
          displayName: "Deploy SonarQube"

        - script: |
            set -e
            export KUBECONFIG="$HOME/.kube/config"

            echo "====================================="
            echo " Install Nexus Repository Manager"
            echo "====================================="
            helm upgrade --install nexus sonatype/nexus-repository-manager \
              --namespace nexus \
              --create-namespace \
              --set nexusBackup.persistence.storageSize=10Gi \
              --set persistence.storageSize=20Gi \
              --force \
              --wait \
              --timeout 15m

            kubectl apply -f ./nexus-ingress.yaml
          displayName: "Deploy Nexus"

        - script: |
            set -e
            export KUBECONFIG="$HOME/.kube/config"

            echo "====================================="
            echo " Install Application (nti-app)"
            echo "====================================="
            helm upgrade --install nti-app ./$(HELM_WORKING_DIR)/nti-app \
              --namespace nti-app \
              --create-namespace \
              --values ./$(HELM_WORKING_DIR)/nti-app/values-nonprod.yaml \
              --force \
              --wait \
              --timeout 10m

            echo "Helm Deployment Completed!"
          displayName: "Deploy Application (nti-app)"

        - script: |
            set -e
            echo "====================================="
            echo " Verify All Deployments"
            echo "====================================="

            export KUBECONFIG="$HOME/.kube/config"

            echo "All Namespaces:"
            kubectl get ns

            echo ""
            echo "===== Nginx Ingress Controller ====="
            kubectl get pods -n ingress-nginx || true
            kubectl get svc -n ingress-nginx || true

            echo ""
            echo "===== ArgoCD ====="
            kubectl get pods -n argo || true
            kubectl get svc -n argo || true
            kubectl get ingress -n argo || true

            echo ""
            echo "===== Vault ====="
            kubectl get pods -n vault || true
            kubectl get svc -n vault || true
            kubectl get ingress -n vault || true

            echo ""
            echo "===== SonarQube ====="
            kubectl get pods -n sonarqube || true
            kubectl get svc -n sonarqube || true
            kubectl get ingress -n sonarqube || true

            echo ""
            echo "===== Nexus ====="
            kubectl get pods -n nexus || true
            kubectl get svc -n nexus || true
            kubectl get ingress -n nexus || true

            echo ""
            echo "===== NTI App ====="
            kubectl get pods -n nti-app || true
            kubectl get svc -n nti-app || true
            kubectl get ingress -n nti-app || true
          displayName: "Verify All Deployments"


# ================================
# Terraform Destroy Stage
# ================================
- stage: Terraform_Destroy
  displayName: "Terraform Destroy Stage"
  dependsOn: Terraform_Apply
  condition: and(succeeded(), eq(variables['RUN_DESTROY'], 'true'))

  jobs:
    - job: CleanupK8s
      displayName: "Cleanup Kubernetes & Helm"
      timeoutInMinutes: 30
      cancelTimeoutInMinutes: 5

      workspace:
        clean: all

      steps:
        - checkout: self

        - task: HelmInstaller@1
          displayName: "Install Helm"
          inputs:
            helmVersionToInstall: "latest"

        - task: KubectlInstaller@0
          displayName: "Install kubectl"
          inputs:
            kubectlVersion: "latest"

        - script: |
            set -e
            echo "====================================="
            echo " Configure kubeconfig for cleanup"
            echo "====================================="

            cd $(TF_WORKING_DIR)
            terraform init -reconfigure

            CLUSTER_NAME=$(terraform output -raw eks_cluster_name)
            echo "Cluster Name: $CLUSTER_NAME"

            mkdir -p $HOME/.kube

            aws eks update-kubeconfig \
              --region "$(AWS_DEFAULT_REGION)" \
              --name "$CLUSTER_NAME" \
              --kubeconfig "$HOME/.kube/config"

            export KUBECONFIG="$HOME/.kube/config"
            kubectl get nodes
          displayName: "Configure kubeconfig"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

        - script: |
            set +e
            echo "====================================="
            echo " Cleanup Helm Releases + Namespaces"
            echo "====================================="

            export KUBECONFIG="$HOME/.kube/config"

            echo "Uninstalling Helm Releases..."
            helm uninstall nti-app -n nti-app || true
            helm uninstall ingress-nginx -n ingress-nginx || true
            helm uninstall argocd -n argo || true
            helm uninstall vault -n vault || true
            helm uninstall sonarqube -n sonarqube || true
            helm uninstall nexus -n nexus || true

            echo "Deleting Namespaces..."
            kubectl delete namespace nti-app --ignore-not-found=true
            kubectl delete namespace ingress-nginx --ignore-not-found=true
            kubectl delete namespace argo --ignore-not-found=true
            kubectl delete namespace vault --ignore-not-found=true
            kubectl delete namespace sonarqube --ignore-not-found=true
            kubectl delete namespace nexus --ignore-not-found=true

            echo "Cleanup done."
          displayName: "Cleanup Helm + Namespaces"

    - job: TerraformDestroy
      displayName: "Terraform Destroy"
      dependsOn: CleanupK8s
      condition: always()
      timeoutInMinutes: 60
      cancelTimeoutInMinutes: 5

      workspace:
        clean: all

      steps:
        - checkout: self

        - script: |
            set -e
            echo "====================================="
            echo " Terraform Destroy"
            echo "====================================="

            cd $(TF_WORKING_DIR)
            terraform init -reconfigure
            terraform destroy -var-file=nonprod.tfvars -auto-approve
          displayName: "Terraform Destroy"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)
