trigger:
  branches:
    include:
      - main

pool:
  name: SelfHostedPool
  demands:
    - agent.os -equals Linux

variables:
  TF_WORKING_DIR: "terraform"
  HELM_WORKING_DIR: "helm"

  AWS_DEFAULT_REGION: "us-east-1"

  TF_STATE_BUCKET: "nti-finalproject-terraform-state"
  TF_LOCK_TABLE: "nti-finalproject-terraform-lock"

  DEPLOY_HELM: "true"
  RUN_DESTROY: "false"

stages:

# ================================
# Terraform Apply Stage
# ================================
- stage: Terraform_Apply
  displayName: "Terraform Apply Stage"
  jobs:
    - job: TerraformApply
      displayName: "Terraform Apply"
      timeoutInMinutes: 60
      cancelTimeoutInMinutes: 5

      workspace:
        clean: all

      steps:
        - checkout: self

        - script: |
            echo "====================================="
            echo " Agent Health Check"
            echo "====================================="
            echo "Agent Name: $(Agent.Name)"
            echo "Agent OS: $(Agent.OS)"
            echo "Build ID: $(Build.BuildId)"
            df -h
            free -h || true
          displayName: "Agent Health Check"

        - script: |
            echo "====================================="
            echo " Verify Tools"
            echo "====================================="
            aws --version
            terraform --version
            git --version
          displayName: "Verify Tools"

        - script: |
            echo "====================================="
            echo " Validate AWS Credentials"
            echo "====================================="
            aws sts get-caller-identity
          displayName: "Validate AWS Credentials"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

        - script: |
            set -e
            echo "====================================="
            echo " Setup Backend Resources"
            echo "====================================="

            echo "Checking S3 bucket: $(TF_STATE_BUCKET)"
            if aws s3api head-bucket --bucket $(TF_STATE_BUCKET) 2>/dev/null; then
              echo "S3 bucket exists"
            else
              echo "Creating S3 bucket..."
              aws s3api create-bucket --bucket $(TF_STATE_BUCKET) --region $(AWS_DEFAULT_REGION)
              aws s3api put-bucket-versioning --bucket $(TF_STATE_BUCKET) --versioning-configuration Status=Enabled
            fi

            echo "Checking DynamoDB table: $(TF_LOCK_TABLE)"
            if aws dynamodb describe-table --table-name $(TF_LOCK_TABLE) --region $(AWS_DEFAULT_REGION) >/dev/null 2>&1; then
              echo "DynamoDB table exists"
            else
              echo "Creating DynamoDB table..."
              aws dynamodb create-table \
                --table-name $(TF_LOCK_TABLE) \
                --attribute-definitions AttributeName=LockID,AttributeType=S \
                --key-schema AttributeName=LockID,KeyType=HASH \
                --billing-mode PAY_PER_REQUEST \
                --region $(AWS_DEFAULT_REGION)

              echo "Waiting for DynamoDB table..."
              aws dynamodb wait table-exists --table-name $(TF_LOCK_TABLE) --region $(AWS_DEFAULT_REGION)
            fi
          displayName: "Setup Backend Resources"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

        - script: |
            set -e
            cd $(TF_WORKING_DIR)

            echo "====================================="
            echo " Terraform Init"
            echo "====================================="
            terraform init -reconfigure
          displayName: "Terraform Init"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

        - script: |
            set -e
            cd $(TF_WORKING_DIR)

            echo "====================================="
            echo " Terraform Validate"
            echo "====================================="
            terraform validate
          displayName: "Terraform Validate"

        - script: |
            set -e
            cd $(TF_WORKING_DIR)

            echo "====================================="
            echo " Terraform Plan"
            echo "====================================="
            terraform plan -var-file=nonprod.tfvars -out=tfplan
          displayName: "Terraform Plan"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

        - script: |
            set -e
            cd $(TF_WORKING_DIR)

            echo "====================================="
            echo " Terraform Apply"
            echo "====================================="
            terraform apply -auto-approve tfplan
          displayName: "Terraform Apply"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

        - script: |
            cd $(TF_WORKING_DIR)
            echo "====================================="
            echo " Terraform Outputs"
            echo "====================================="
            terraform output
          displayName: "Terraform Outputs"


# ================================
# Helm Deploy Stage
# ================================
- stage: Helm_Deploy
  displayName: "Helm Deployment Stage"
  dependsOn: Terraform_Apply
  condition: and(succeeded(), eq(variables['DEPLOY_HELM'], 'true'))

  jobs:
    - job: HelmDeploy
      displayName: "Helm Deploy"
      timeoutInMinutes: 40
      cancelTimeoutInMinutes: 5

      workspace:
        clean: all

      steps:
        - checkout: self

        - task: HelmInstaller@1
          displayName: "Install Helm"
          inputs:
            helmVersionToInstall: "latest"

        - task: KubectlInstaller@0
          displayName: "Install kubectl"
          inputs:
            kubectlVersion: "latest"

        - script: |
            set -e
            echo "====================================="
            echo " Configure kubeconfig for EKS"
            echo "====================================="

            cd $(TF_WORKING_DIR)
            terraform init -reconfigure

            CLUSTER_NAME=$(terraform output -raw eks_cluster_name)
            echo "Cluster Name: $CLUSTER_NAME"

            echo "Creating kubeconfig directory..."
            mkdir -p $HOME/.kube

            echo "Updating kubeconfig..."
            aws eks update-kubeconfig \
              --region "$(AWS_DEFAULT_REGION)" \
              --name "$CLUSTER_NAME" \
              --kubeconfig "$HOME/.kube/config"

            echo "Testing kubectl..."
            export KUBECONFIG="$HOME/.kube/config"
            kubectl cluster-info
            kubectl get nodes
          displayName: "Configure kubectl for EKS"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

        - script: |
            echo "====================================="
            echo " Check Helm Chart Folder"
            echo "====================================="
            ls -la
            ls -la $(HELM_WORKING_DIR)
            ls -la $(HELM_WORKING_DIR)/nti-app
          displayName: "Check Helm Chart Files"

        - script: |
            set -e
            echo "====================================="
            echo " Helm Upgrade/Install"
            echo "====================================="

            export KUBECONFIG="$HOME/.kube/config"
            echo "KUBECONFIG=$KUBECONFIG"

            kubectl get nodes

            helm upgrade --install nti-app ./$(HELM_WORKING_DIR)/nti-app \
              --namespace nti-app \
              --create-namespace \
              --values ./$(HELM_WORKING_DIR)/nti-app/values-nonprod.yaml \
              --wait \
              --timeout 10m

            echo "Helm Deployment Completed!"
          displayName: "Helm Upgrade/Install"

        - script: |
            set -e
            echo "====================================="
            echo " Verify Deployment"
            echo "====================================="

            export KUBECONFIG="$HOME/.kube/config"

            kubectl get ns
            kubectl get pods -n nti-app
            kubectl get svc -n nti-app
            kubectl get ingress -n nti-app || true
          displayName: "Verify Deployment"


# ================================
# Terraform Destroy Stage
# ================================
- stage: Terraform_Destroy
  displayName: "Terraform Destroy Stage"
  dependsOn: Terraform_Apply
  condition: and(succeeded(), eq(variables['RUN_DESTROY'], 'true'))

  jobs:
    - job: CleanupK8s
      displayName: "Cleanup Kubernetes & Helm"
      timeoutInMinutes: 30
      cancelTimeoutInMinutes: 5

      workspace:
        clean: all

      steps:
        - checkout: self

        - task: HelmInstaller@1
          displayName: "Install Helm"
          inputs:
            helmVersionToInstall: "latest"

        - task: KubectlInstaller@0
          displayName: "Install kubectl"
          inputs:
            kubectlVersion: "latest"

        - script: |
            set -e
            echo "====================================="
            echo " Configure kubeconfig for cleanup"
            echo "====================================="

            cd $(TF_WORKING_DIR)
            terraform init -reconfigure

            CLUSTER_NAME=$(terraform output -raw eks_cluster_name)
            echo "Cluster Name: $CLUSTER_NAME"

            mkdir -p $HOME/.kube

            aws eks update-kubeconfig \
              --region "$(AWS_DEFAULT_REGION)" \
              --name "$CLUSTER_NAME" \
              --kubeconfig "$HOME/.kube/config"

            export KUBECONFIG="$HOME/.kube/config"
            kubectl get nodes
          displayName: "Configure kubeconfig"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

        - script: |
            set +e
            echo "====================================="
            echo " Cleanup Helm Releases + Namespaces"
            echo "====================================="

            export KUBECONFIG="$HOME/.kube/config"

            helm uninstall nti-app -n nti-app || true
            helm uninstall ingress-nginx -n ingress-nginx || true
            helm uninstall argocd -n argo || true

            kubectl delete namespace nti-app --ignore-not-found=true
            kubectl delete namespace ingress-nginx --ignore-not-found=true
            kubectl delete namespace argo --ignore-not-found=true
            kubectl delete namespace nexus --ignore-not-found=true
            kubectl delete namespace vault --ignore-not-found=true
            kubectl delete namespace sonarqube --ignore-not-found=true

            echo "Cleanup done."
          displayName: "Cleanup Helm + Namespaces"

    - job: TerraformDestroy
      displayName: "Terraform Destroy"
      dependsOn: CleanupK8s
      condition: always()
      timeoutInMinutes: 60
      cancelTimeoutInMinutes: 5

      workspace:
        clean: all

      steps:
        - checkout: self

        - script: |
            set -e
            echo "====================================="
            echo " Terraform Destroy"
            echo "====================================="

            cd $(TF_WORKING_DIR)
            terraform init -reconfigure
            terraform destroy -var-file=nonprod.tfvars -auto-approve
          displayName: "Terraform Destroy"
          env:
            AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
            AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
            AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)
