trigger:
  branches:
    include:
      - main

pool:
  name: SelfHostedPool
  demands:
    - agent.os -equals Linux
    - Agent.Version -gtVersion 2.150.0

variables:
  TF_WORKING_DIR: "terraform"
  HELM_WORKING_DIR: "helm"
  PROJECT_NAME: "nti-final"

  AWS_DEFAULT_REGION: "us-east-1"

  RUN_DESTROY: "false"
  DEPLOY_HELM: "true"
  RUN_TESTS: "true"
  MANUAL_TOOL_INSTALL: "false"

stages:
  - stage: Terraform_Apply
    displayName: "Terraform Apply Stage"
    jobs:
      - job: Apply
        displayName: "Terraform Apply"
        timeoutInMinutes: 60
        cancelTimeoutInMinutes: 5

        workspace:
          clean: all

        steps:
          - checkout: self

          - script: |
              echo "Pipeline started on agent: $(Agent.Name)"
              echo "Agent OS: $(Agent.OS)"
              echo "Build ID: $(Build.BuildId)"
              echo "Build Number: $(Build.BuildNumber)"
              df -h
              free -h || echo "Free command not available"
            displayName: "Agent Health Check"

          - script: |
              echo "Verifying AWS CLI..."
              aws --version
              echo "Verifying Terraform..."
              terraform --version
              echo "Verifying Git..."
              git --version
            displayName: "Verify Required Tools"

          - script: |
              echo "Validating AWS credentials..."
              aws sts get-caller-identity
            displayName: "Validate AWS Credentials"
            env:
              AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
              AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

          - script: |
              set -e
              echo "Setting up AWS backend resources..."

              echo "Checking S3 bucket: nti-finalproject-terraform-state"
              if aws s3 ls s3://nti-finalproject-terraform-state 2>/dev/null; then
                echo "S3 bucket already exists"
              else
                echo "Creating S3 bucket..."
                aws s3 mb s3://nti-finalproject-terraform-state --region us-east-1
                aws s3api put-bucket-versioning --bucket nti-finalproject-terraform-state --versioning-configuration Status=Enabled
              fi

              echo "Checking DynamoDB table: nti-finalproject-terraform-lock"
              if aws dynamodb describe-table --table-name nti-finalproject-terraform-lock --region us-east-1 2>/dev/null; then
                echo "DynamoDB table already exists"
              else
                echo "Creating DynamoDB table..."
                aws dynamodb create-table \
                  --table-name nti-finalproject-terraform-lock \
                  --attribute-definitions AttributeName=LockID,AttributeType=S \
                  --key-schema AttributeName=LockID,KeyType=HASH \
                  --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5 \
                  --region us-east-1

                echo "Waiting for DynamoDB table to be active..."
                aws dynamodb wait table-exists --table-name nti-finalproject-terraform-lock --region us-east-1
              fi

              echo "Backend resources are ready."
              cd $(TF_WORKING_DIR)
              terraform init -reconfigure
            displayName: "Setup Backend and Terraform Init"
            env:
              AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
              AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

          - script: |
              cd $(TF_WORKING_DIR)
              echo "Running Terraform validate..."
              terraform validate
            displayName: "Terraform Validate"

          - script: |
              cd $(TF_WORKING_DIR)
              echo "Running Terraform plan..."
              terraform plan -var-file=nonprod.tfvars -out=tfplan
            displayName: "Terraform Plan"
            env:
              AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
              AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

          - script: |
              cd $(TF_WORKING_DIR)
              echo "Applying Terraform plan..."
              terraform apply -auto-approve tfplan
            displayName: "Terraform Apply"
            env:
              AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
              AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

          - script: |
              cd $(TF_WORKING_DIR)
              terraform output
            displayName: "Terraform Outputs"
            env:
              AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
              AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

  - stage: Helm_Deploy
    displayName: "Helm Application Deployment"
    dependsOn: Terraform_Apply
    condition: and(succeeded(), eq(variables['DEPLOY_HELM'], 'true'))

    jobs:
      - job: Deploy
        displayName: "Deploy Applications with Helm"
        timeoutInMinutes: 30
        cancelTimeoutInMinutes: 5

        variables:
          KUBECONFIG: $(Agent.TempDirectory)/.kube/config

        steps:
          - checkout: self

          - task: HelmInstaller@1
            displayName: "Install Helm"
            inputs:
              helmVersionToInstall: "latest"

          - task: KubectlInstaller@0
            displayName: "Install kubectl"
            inputs:
              kubectlVersion: "latest"

          - script: |
              set -e
              echo "Configuring kubectl for EKS..."

              cd $(TF_WORKING_DIR)

              terraform init -reconfigure

              echo "Getting cluster name from terraform output..."
              CLUSTER_NAME=$(terraform output -raw eks_cluster_name)

              echo "Cluster name: $CLUSTER_NAME"

              echo "Checking cluster exists..."
              aws eks describe-cluster --name $CLUSTER_NAME --region $(AWS_DEFAULT_REGION)

              echo "Updating kubeconfig..."
              mkdir -p ~/.kube
              aws eks update-kubeconfig --region $(AWS_DEFAULT_REGION) --name $CLUSTER_NAME

              echo "Testing kubectl..."
              kubectl cluster-info
              kubectl get nodes
            displayName: "Configure kubectl for EKS"
            env:
              AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
              AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

          - script: |
              echo "Checking Helm chart folder..."
              ls -la
              ls -la helm
              ls -la helm/nti-app
            displayName: "Check Helm Chart Files"

          - script: |
              set -e
              echo "====================================="
              echo " Deploy Helm Chart"
              echo "====================================="

              export KUBECONFIG="$HOME/.kube/config"
              echo "KUBECONFIG=$KUBECONFIG"

              helm upgrade --install nti-app ./$(HELM_WORKING_DIR)/nti-app \
                --namespace nti-app \
                --create-namespace \
                --values ./$(HELM_WORKING_DIR)/nti-app/values-nonprod.yaml \
                --wait \
                --timeout 10m

              echo "Helm Deployment Completed!"
            displayName: "Helm Upgrade/Install"
            env:
              KUBECONFIG: $(KUBECONFIG)

          - script: |
              echo "=== Verifying Deployment ==="
              kubectl get ns
              kubectl get pods -n nti-app
              kubectl get svc -n nti-app
              kubectl get ingress -n nti-app || true
            displayName: "Verify Deployment"

  - stage: Terraform_Destroy
    displayName: "Terraform Destroy Stage (Manual)"
    dependsOn: Terraform_Apply
    condition: and(succeeded(), eq(variables['RUN_DESTROY'], 'true'))

    jobs:
      - job: Destroy
        displayName: "Terraform Destroy"
        timeoutInMinutes: 30
        cancelTimeoutInMinutes: 5

        steps:
          - checkout: self

          - script: |
              cd $(TF_WORKING_DIR)
              terraform init -reconfigure
            displayName: "Terraform Init (Destroy)"
            env:
              AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
              AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

          - script: |
              cd $(TF_WORKING_DIR)
              terraform destroy -var-file=nonprod.tfvars -auto-approve
            displayName: "Terraform Destroy"
            env:
              AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
              AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)
