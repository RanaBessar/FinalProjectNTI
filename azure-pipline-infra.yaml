trigger:
  branches:
    include:
      - main

# Optimized agent pool configuration
pool:
  name: SelfHostedPool
  demands:
  - agent.os -equals Linux
  - Agent.Version -gtVersion 2.150.0

variables:
  # Project configuration
  TF_WORKING_DIR: "terraform"
  HELM_WORKING_DIR: "helm"
  PROJECT_NAME: "nti-final"
  
  # AWS configuration
  AWS_DEFAULT_REGION: "us-east-1"
  
  # Pipeline control flags
  RUN_DESTROY: "false"
  DEPLOY_HELM: "true"
  RUN_TESTS: "true"
  MANUAL_TOOL_INSTALL: "false"  # Use extensions by default
  
  # Version pinning for stability
  TERRAFORM_VERSION: "1.7.0"
  KUBECTL_VERSION: "latest"
  HELM_VERSION: "latest"

stages:
  - stage: Terraform_Apply
    displayName: "Terraform Apply Stage"
    jobs:
      - job: Apply
        displayName: "Terraform Apply"
        timeoutInMinutes: 60  # Prevent hanging jobs
        cancelTimeoutInMinutes: 5
        # Strategy to handle agent queue issues
        strategy:
          maxParallel: 1  # Run one at a time to avoid resource conflicts
        
        # Workspace cleanup for self-hosted agents
        workspace:
          clean: all  # Clean workspace between runs
        steps:
          - checkout: self

          # Check agent health and capacity
          - script: |
              echo "Pipeline started on agent: $(Agent.Name)"
              echo "Agent OS: $(Agent.OS)"
              echo "Build ID: $(Build.BuildId)"
              echo "Build Number: $(Build.BuildNumber)"
              df -h
              free -h || echo "Free command not available"
            displayName: "Agent Health Check"

          # Verify required tools (should be pre-installed on self-hosted agents)
          - script: |
              echo "Verifying AWS CLI..."
              aws --version
              echo "Verifying Terraform..."
              terraform --version
              echo "Verifying Git..."
              git --version
            displayName: "Verify Required Tools"

          # Validate AWS credentials
          - script: |
              echo "Validating AWS credentials..."
              aws sts get-caller-identity
            displayName: "Validate AWS Credentials"
            env:
              AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
              AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

          - script: |
              set -e  # Exit on any error
              echo "Setting up AWS backend resources..."
              
              # Check if S3 bucket exists, create if not
              echo "Checking S3 bucket: nti-finalproject-terraform-state"
              if aws s3 ls s3://nti-finalproject-terraform-state 2>/dev/null; then
                echo "S3 bucket already exists"
              else
                echo "Creating S3 bucket..."
                aws s3 mb s3://nti-finalproject-terraform-state --region us-east-1
                # Enable versioning for better state management
                aws s3api put-bucket-versioning --bucket nti-finalproject-terraform-state --versioning-configuration Status=Enabled
              fi
              
              # Check if DynamoDB table exists, create if not
              echo "Checking DynamoDB table: nti-finalproject-terraform-lock"
              if aws dynamodb describe-table --table-name nti-finalproject-terraform-lock --region us-east-1 2>/dev/null; then
                echo "DynamoDB table already exists"
              else
                echo "Creating DynamoDB table..."
                aws dynamodb create-table \
                  --table-name nti-finalproject-terraform-lock \
                  --attribute-definitions AttributeName=LockID,AttributeType=S \
                  --key-schema AttributeName=LockID,KeyType=HASH \
                  --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5 \
                  --region us-east-1
                
                # Wait for table to be active
                echo "Waiting for DynamoDB table to be active..."
                aws dynamodb wait table-exists --table-name nti-finalproject-terraform-lock --region us-east-1
              fi
              
              echo "Backend resources are ready. Initializing Terraform..."
              cd $(TF_WORKING_DIR)
              terraform init -reconfigure
            displayName: "Setup Backend and Terraform Init"
            env:
              AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
              AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

          - script: |
              cd $(TF_WORKING_DIR)
              echo "Running Terraform validate..."
              terraform validate
              
              # Additional validation checks
              echo "Checking for required outputs..."
              terraform show -json tfplan | jq '.planned_values.outputs | keys[]' || echo "No outputs in plan"
            displayName: "Terraform Validate & Checks"

          # Test and validate stage
          - script: |
              cd $(TF_WORKING_DIR)
              echo "Running security and compliance checks..."
              
              # Check for required tags
              echo "Validating resource tagging..."
              terraform show -json tfplan | jq '.planned_values.root_module.resources[] | select(.values.tags == null) | .address' || echo "All resources have tags"
              
              # Validate naming conventions
              echo "Validating naming conventions..."
              terraform show -json tfplan | jq '.planned_values.root_module.resources[] | select(.name | test("^[a-z0-9-]+$") | not) | .address' || echo "Naming conventions OK"
              
              echo "Validation checks completed"
            displayName: "Security & Compliance Checks"
            condition: eq(variables['RUN_TESTS'], 'true')

          - script: |
              cd $(TF_WORKING_DIR)
              echo "Running Terraform plan..."
              terraform plan -var-file=nonprod.tfvars -out=tfplan
              echo "Plan saved to tfplan file"
              
              # Show plan summary
              echo "=== Plan Summary ==="
              terraform show -no-color tfplan | head -50
            displayName: "Terraform Plan"
            env:
              AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
              AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

          - script: |
              cd $(TF_WORKING_DIR)
              echo "Applying Terraform plan..."
              terraform apply tfplan
              echo "Terraform apply completed successfully"
            displayName: "Terraform Apply"
            env:
              AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
              AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

          - script: |
              cd $(TF_WORKING_DIR)
              terraform output
            displayName: "Terraform Outputs"
            env:
              AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
              AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

  # Helm Deployment Stage
  - stage: Helm_Deploy
    displayName: "Helm Application Deployment"
    dependsOn: Terraform_Apply
    condition: and(succeeded(), eq(variables['DEPLOY_HELM'], 'true'))
    jobs:
      - job: Deploy
        displayName: "Deploy Applications with Helm"
        timeoutInMinutes: 30
        cancelTimeoutInMinutes: 5
        variables:
          KUBECONFIG: $(Agent.TempDirectory)/.kube/config
        steps:
          - checkout: self

          # Manual Install Option (if extensions not available)
          - script: |
              # Install kubectl manually
              if ! command -v kubectl &> /dev/null; then
                echo "Installing kubectl..."
                curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl" 
                chmod +x kubectl
                sudo mv kubectl /usr/local/bin/
              fi
              kubectl version --client
              
              # Install Helm manually
              if ! command -v helm &> /dev/null; then
                echo "Installing Helm..."
                curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
              fi
              helm version
            displayName: "Manual Tool Install (Backup)"
            condition: eq(variables['MANUAL_TOOL_INSTALL'], 'true')

          # Install Helm using Azure DevOps Extension (Preferred)
          - task: HelmInstaller@1
            displayName: 'Install Helm'
            condition: ne(variables['MANUAL_TOOL_INSTALL'], 'true')
            inputs:
              helmVersionToInstall: 'latest'

          # Install kubectl using Azure DevOps Extension (Preferred)
          - task: KubectlInstaller@0
            displayName: 'Install kubectl'  
            condition: ne(variables['MANUAL_TOOL_INSTALL'], 'true')
            inputs:
              kubectlVersion: 'latest'

          # Configure kubectl for EKS
          - script: |
              echo "Configuring kubectl for EKS cluster..."
              
              # Ensure we're in the right directory and terraform is initialized
              cd $(TF_WORKING_DIR)
              
              # Initialize terraform to read outputs
              echo "Initializing Terraform to read outputs..."
              terraform init -reconfigure
              
              # Get cluster name from Terraform output with error handling
              echo "Getting cluster information from Terraform..."
              if terraform output cluster_name > /dev/null 2>&1; then
                CLUSTER_NAME=$(terraform output -raw cluster_name)
                echo "Found cluster name: $CLUSTER_NAME"
              else
                echo "Warning: Could not get cluster name from Terraform output"
                CLUSTER_NAME="nti-final-nonprod"
                echo "Using fallback cluster name: $CLUSTER_NAME"
              fi
              
              # Verify cluster exists before configuring kubectl
              echo "Verifying cluster exists..."
              if aws eks describe-cluster --name $CLUSTER_NAME --region $(AWS_DEFAULT_REGION) > /dev/null 2>&1; then
                echo "Cluster $CLUSTER_NAME found successfully"
                
                # Update kubeconfig
                echo "Updating kubeconfig..."
                aws eks update-kubeconfig \
                  --region $(AWS_DEFAULT_REGION) \
                  --name $CLUSTER_NAME
                
                # Set KUBECONFIG environment variable for subsequent tasks
                echo "##vso[task.setvariable variable=KUBECONFIG]$HOME/.kube/config"
                
                # Verify connection
                echo "Verifying cluster connection..."
                kubectl cluster-info
                kubectl get nodes
                kubectl get namespaces
                
              else
                echo "##[error]Cluster $CLUSTER_NAME not found in region $(AWS_DEFAULT_REGION)"
                echo "Available clusters:"
                aws eks list-clusters --region $(AWS_DEFAULT_REGION) --query 'clusters' --output table
                exit 1
              fi
            displayName: "Configure kubectl for EKS"
            env:
              AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
              AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

          # Deploy with Helm using Azure DevOps extension (Preferred)
          - task: HelmDeploy@0
            displayName: 'Deploy Application with Helm'
            condition: ne(variables['MANUAL_TOOL_INSTALL'], 'true')
            inputs:
              connectionType: 'None'  # For EKS with direct kubectl access
              kubernetesServiceConnection: ''
              command: 'upgrade'
              chartType: 'FilePath'
              chartPath: '$(HELM_WORKING_DIR)/nti-app'
              releaseName: 'nti-app'
              valueFile: '$(HELM_WORKING_DIR)/nti-app/values-nonprod.yaml'
              arguments: '--install --wait --timeout 10m --create-namespace --namespace nti-app'
            env:
              KUBECONFIG: $(KUBECONFIG)
          
          # Manual Helm deployment (if extension not available)
          - script: |
              echo "Deploying with Helm manually..."
              
              # Ensure KUBECONFIG is set
              if [ -z "$KUBECONFIG" ]; then
                export KUBECONFIG="$HOME/.kube/config"
                echo "Set KUBECONFIG to $KUBECONFIG"
              fi
              
              # Verify kubectl connection before Helm deployment
              echo "Verifying kubectl connection..."
              if ! kubectl cluster-info > /dev/null 2>&1; then
                echo "##[error]kubectl is not properly configured. Cannot deploy with Helm."
                exit 1
              fi
              
              cd $(HELM_WORKING_DIR)
              helm upgrade nti-app ./nti-app \
                --install \
                --wait \
                --timeout 10m \
                --create-namespace \
                --namespace nti-app \
                --values ./nti-app/values-nonprod.yaml
            displayName: 'Deploy with Helm (Manual)'
            condition: eq(variables['MANUAL_TOOL_INSTALL'], 'true')
            env:
              KUBECONFIG: $(KUBECONFIG)
              
          # Verify deployment and run tests
          - script: |
              echo "=== Verifying Helm deployment ==="
              helm list -n nti-app
              
              echo ""
              echo "=== Checking pod status ==="
              kubectl get pods -n nti-app
              kubectl wait --for=condition=Ready pod -l app.kubernetes.io/name=nti-app -n nti-app --timeout=300s
              
              echo ""
              echo "=== Checking services ==="
              kubectl get svc -n nti-app
              
              echo ""
              echo "=== Checking ingress ==="
              kubectl get ingress -n nti-app
              
              # Run application tests if enabled
              if [ "$(RUN_TESTS)" = "true" ]; then
                echo ""
                echo "=== Running application health tests ==="
                
                # Get service endpoint
                SERVICE_IP=$(kubectl get svc nti-app -n nti-app -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' || echo "pending")
                if [ "$SERVICE_IP" != "pending" ] && [ "$SERVICE_IP" != "" ]; then
                  echo "Testing health endpoint: $SERVICE_IP"
                  curl -f "http://$SERVICE_IP/health" || echo "Health check endpoint not responding (this is expected for nginx default)"
                else
                  echo "LoadBalancer IP not yet assigned, skipping endpoint tests"
                fi
                
                # Test pod logs
                echo "Checking application logs..."
                kubectl logs -l app.kubernetes.io/name=nti-app -n nti-app --tail=10 || echo "No logs available yet"
                
                # Test HPA if enabled
                HPA_EXISTS=$(kubectl get hpa nti-app -n nti-app -o name 2>/dev/null || echo "")
                if [ "$HPA_EXISTS" != "" ]; then
                  echo "Checking HPA status..."
                  kubectl get hpa -n nti-app
                else
                  echo "HPA not configured"
                fi
              fi
            displayName: "Verify Deployment & Run Tests"

          # Deployment summary and connection info
          - script: |
              echo ""
              echo "ðŸŽ‰ =============================================="
              echo "ðŸŽ‰    DEPLOYMENT COMPLETED SUCCESSFULLY!      "
              echo "ðŸŽ‰ =============================================="
              echo ""
              
              # Get cluster info
              cd $(TF_WORKING_DIR)
              CLUSTER_NAME=$(terraform output -raw cluster_name 2>/dev/null || echo "nti-final-nonprod")
              CLUSTER_ENDPOINT=$(terraform output -raw cluster_endpoint 2>/dev/null || echo "Not available")
              ECR_REPO=$(terraform output -raw ecr_repository_url 2>/dev/null || echo "Not available")
              
              echo "ðŸ“‹ INFRASTRUCTURE DETAILS:"
              echo "   ðŸ—ï¸  EKS Cluster: $CLUSTER_NAME"
              echo "   ðŸŒ Endpoint: $CLUSTER_ENDPOINT"
              echo "   ðŸ“¦ ECR Repository: $ECR_REPO"
              echo "   ðŸŒ Region: $(AWS_DEFAULT_REGION)"
              echo ""
              
              echo "ðŸ“¦ APPLICATION DETAILS:"
              echo "   ðŸ“± Application: nti-app"
              echo "   ðŸ·ï¸  Namespace: nti-app"
              echo "   ðŸ“Š Replicas: $(kubectl get deployment nti-app -n nti-app -o jsonpath='{.spec.replicas}' 2>/dev/null || echo 'N/A')"
              
              # Get LoadBalancer info
              LB_HOSTNAME=$(kubectl get svc nti-app -n nti-app -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
              if [ "$LB_HOSTNAME" != "" ]; then
                echo "   ðŸŒ LoadBalancer: $LB_HOSTNAME"
                echo "   ðŸ”— Application URL: http://$LB_HOSTNAME"
              else
                echo "   ðŸ”„ LoadBalancer: Provisioning..."
              fi
              
              # Get Ingress info
              INGRESS_HOST=$(kubectl get ingress nti-app -n nti-app -o jsonpath='{.spec.rules[0].host}' 2>/dev/null || echo "")
              if [ "$INGRESS_HOST" != "" ]; then
                echo "   ðŸšª Ingress Host: $INGRESS_HOST"
              fi
              
              echo ""
              echo "ðŸ”§ CONNECTION COMMANDS:"
              echo "   kubectl: aws eks update-kubeconfig --region $(AWS_DEFAULT_REGION) --name $CLUSTER_NAME"
              echo "   Helm: helm list -n nti-app"
              echo "   Logs: kubectl logs -f deployment/nti-app -n nti-app"
              echo ""
              echo "ðŸ“Š MONITORING COMMANDS:"
              echo "   Pods: kubectl get pods -n nti-app -w"
              echo "   HPA: kubectl get hpa -n nti-app"
              echo "   Events: kubectl get events -n nti-app --sort-by=.metadata.creationTimestamp"
              echo ""
              echo "âœ¨ Your application is now running on AWS EKS! âœ¨"
              echo "=============================================="
            displayName: "ðŸŽ‰ Deployment Summary"

  - stage: Terraform_Destroy
    displayName: "Terraform Destroy Stage (Manual)"
    dependsOn: Terraform_Apply
    # Only run destroy if explicitly requested via pipeline variable
    condition: and(succeeded(), or(eq(variables['RUN_DESTROY'], 'true'), eq(variables['Build.Reason'], 'Manual')))
    jobs:
      # First cleanup Helm deployments
      - job: CleanupHelm
        displayName: "Cleanup Helm Deployments"
        condition: eq(variables['DEPLOY_HELM'], 'true')
        steps:
          - checkout: self

          # Manual tool installation (backup option)
          - script: |
              if ! command -v kubectl &> /dev/null; then
                echo "Installing kubectl..."
                curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl" 
                chmod +x kubectl && sudo mv kubectl /usr/local/bin/
              fi
              if ! command -v helm &> /dev/null; then
                echo "Installing Helm..."
                curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
              fi
            displayName: "Manual Tool Install (Cleanup)"
            condition: eq(variables['MANUAL_TOOL_INSTALL'], 'true')

          - task: HelmInstaller@1
            displayName: 'Install Helm'
            condition: ne(variables['MANUAL_TOOL_INSTALL'], 'true')  
            inputs:
              helmVersionToInstall: 'latest'

          - task: KubectlInstaller@0
            displayName: 'Install kubectl'
            condition: ne(variables['MANUAL_TOOL_INSTALL'], 'true')
            inputs:
              kubectlVersion: 'latest'

          # Configure kubectl and cleanup
          - script: |
              echo "Configuring kubectl for EKS cleanup..."
              cd $(TF_WORKING_DIR)
              
              # Initialize terraform to read outputs
              echo "Initializing Terraform..."
              terraform init -reconfigure
              
              # Get cluster name with error handling
              if terraform output cluster_name > /dev/null 2>&1; then
                CLUSTER_NAME=$(terraform output -raw cluster_name)
                echo "Found cluster name: $CLUSTER_NAME"
              else
                echo "Warning: Could not get cluster name from Terraform output"
                CLUSTER_NAME="nti-final-nonprod"
                echo "Using fallback cluster name: $CLUSTER_NAME"
              fi
              
              # Check if cluster exists before cleanup
              if aws eks describe-cluster --name $CLUSTER_NAME --region $(AWS_DEFAULT_REGION) > /dev/null 2>&1; then
                echo "Cluster $CLUSTER_NAME found, proceeding with cleanup"
                
                # Configure kubectl
                aws eks update-kubeconfig \
                  --region $(AWS_DEFAULT_REGION) \
                  --name $CLUSTER_NAME
                
                echo "Removing Helm releases..."
                helm uninstall nti-app -n nti-app || echo "Release not found or already removed"
                
                echo "Cleaning up namespace..."
                kubectl delete namespace nti-app || echo "Namespace not found or already removed"
                
              else
                echo "Cluster $CLUSTER_NAME not found - skipping Kubernetes cleanup"
              fi
            displayName: "Cleanup Helm Deployments"
            env:
              AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
              AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

      # Then destroy infrastructure  
      - job: Destroy
        displayName: "Terraform Destroy"
        dependsOn: CleanupHelm
        condition: always()  # Run even if cleanup fails
        timeoutInMinutes: 30
        cancelTimeoutInMinutes: 5
        steps:
          - checkout: self

          # Verify required tools
          - script: |
              aws --version
              terraform --version
            displayName: "Verify Required Tools"

          - script: |
              cd $(TF_WORKING_DIR)
              terraform init
            displayName: "Terraform Init (Destroy)"
            env:
              AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
              AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

          - script: |
              cd $(TF_WORKING_DIR)
              terraform destroy -var-file=nonprod.tfvars -auto-approve
            displayName: "Terraform Destroy"
            env:
              AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
              AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)
